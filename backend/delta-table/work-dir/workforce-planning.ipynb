{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511d60c4-9d28-4b3a-bbc0-5af546d7053c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ortools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mortools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_solver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywraplp\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ortools'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from ortools.linear_solver import pywraplp\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeTaskAllocation\").getOrCreate()\n",
    "\n",
    "# Generate synthetic data for employees and tasks\n",
    "num_employees = 30  # For debugging, using a smaller number of employees\n",
    "num_tasks = 2  # Assuming 100 tasks\n",
    "locations = [\"Location1\", \"Location2\", \"Location3\"]\n",
    "skills = [\"Skill1\", \"Skill2\", \"Skill3\"]\n",
    "\n",
    "# Create synthetic employees data\n",
    "employees = []\n",
    "for i in range(num_employees):\n",
    "    employees.append({\n",
    "        'id': i,\n",
    "        'locations': random.sample(locations, random.randint(1, len(locations))),  # Random locations\n",
    "        'skills': random.sample(skills, random.randint(1, len(skills))),  # Random skills\n",
    "        'holidays': random.sample(range(1, 31), random.randint(0, 1))  # Random holidays in a month\n",
    "    })\n",
    "\n",
    "# Create synthetic tasks data\n",
    "tasks = []\n",
    "for i in range(num_tasks):\n",
    "    tasks.append({\n",
    "        'id': i,\n",
    "        'location': random.choice(locations),  # Single location for each task\n",
    "        'skills_required': random.sample(skills, random.randint(1, len(skills))),  # Random required skills\n",
    "        'start_date': random.randint(1, 15),  # Random start date in the first half of the month\n",
    "        'duration': random.randint(1, 2),  # Random duration between 1 and 5 days\n",
    "        'working_hours_per_day': random.randint(1, 8)  # Random working hours per day between 1 and 8\n",
    "    })\n",
    "\n",
    "# Convert the synthetic data to Spark DataFrames\n",
    "df_employees = spark.createDataFrame(employees)\n",
    "df_tasks = spark.createDataFrame(tasks)\n",
    "\n",
    "# Broadcast the tasks DataFrame\n",
    "broadcast_tasks = spark.sparkContext.broadcast(df_tasks.collect())\n",
    "\n",
    "\n",
    "# Define the allocation function\n",
    "def allocate_tasks(employee_partition):\n",
    "    from ortools.linear_solver import pywraplp\n",
    "    employees = list(employee_partition)\n",
    "    tasks = broadcast_tasks.value\n",
    "    # Create the solver using SCIP backend\n",
    "    # (other options include GLPK, CBC, etc.)\n",
    "    solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "    num_employees = len(employees)\n",
    "    num_tasks = len(tasks)\n",
    "\n",
    "    if num_employees == 0 or num_tasks == 0:\n",
    "        return []\n",
    "\n",
    "    # Decision variables: x[e, t] is 1 if employee 'e' is assigned to task 't', otherwise 0\n",
    "    assignment_vars = {}\n",
    "    for emp_id in range(num_employees):\n",
    "        for task_id in range(num_tasks):\n",
    "            assignment_vars[emp_id, task_id] = solver.BoolVar(f'x[{emp_id},{task_id}]')\n",
    "\n",
    "    # Constraints: Each task should be assigned to exactly one employee\n",
    "    for task_id in range(num_tasks):\n",
    "        task = tasks[task_id]\n",
    "        solver.Add(solver.Sum(\n",
    "            assignment_vars[emp_id, task_id] for emp_id in range(num_employees)\n",
    "            if task['location'] in employees[emp_id]['locations'] and\n",
    "            all(skill in employees[emp_id]['skills'] for skill in task['skills_required']) and\n",
    "            task['start_date'] not in employees[emp_id]['holidays']\n",
    "        ) == 1)\n",
    "\n",
    "    # Constraints: Each employee can only work on one task at a time and cannot work more than 8 hours a day\n",
    "    for emp_id in range(num_employees):\n",
    "        for day in range(1, 32):  # Assuming a month has 31 days\n",
    "            overlapping_tasks = [\n",
    "                task_id for task_id in range(num_tasks)\n",
    "                if tasks[task_id]['start_date'] <= day < tasks[task_id]['start_date'] + tasks[task_id]['duration']\n",
    "            ]\n",
    "            solver.Add(solver.Sum(\n",
    "                assignment_vars[emp_id, task_id] * tasks[task_id]['working_hours_per_day']\n",
    "                for task_id in overlapping_tasks\n",
    "            ) <= 8)\n",
    "\n",
    "    # Constraints: Each employee cannot work more than 40 hours per week\n",
    "    for emp_id in range(num_employees):\n",
    "        for week in range(5):  # Assuming 4 weeks in a month\n",
    "            weekly_tasks = [\n",
    "                task_id for task_id in range(num_tasks)\n",
    "                if tasks[task_id]['start_date'] // 7 == week or\n",
    "                (tasks[task_id]['start_date'] + tasks[task_id]['duration']) // 7 == week\n",
    "            ]\n",
    "            solver.Add(solver.Sum(\n",
    "                assignment_vars[emp_id, task_id] * tasks[task_id]['working_hours_per_day']\n",
    "                for task_id in weekly_tasks\n",
    "            ) <= 40)\n",
    "\n",
    "    # Objective: Maximize the number of assigned tasks\n",
    "    solver.Maximize(solver.Sum(\n",
    "        assignment_vars[emp_id, task_id] for emp_id in range(num_employees) for task_id in range(num_tasks)\n",
    "    ))\n",
    "\n",
    "    # Solve the problem\n",
    "    status = solver.Solve()\n",
    "\n",
    "    # Check the results\n",
    "    assignments = []\n",
    "    if status == pywraplp.Solver.OPTIMAL:\n",
    "        for emp_id in range(num_employees):\n",
    "            for task_id in range(num_tasks):\n",
    "                if assignment_vars[emp_id, task_id].solution_value() > 0:\n",
    "                    assignments.append((employees[emp_id]['id'], tasks[task_id]['id'], tasks[task_id]['start_date'], tasks[task_id]['duration']))\n",
    "    else:\n",
    "        print(\"Solver did not find an optimal solution\")\n",
    "\n",
    "    return assignments\n",
    "\n",
    "# Apply the allocation function across partitions\n",
    "results = df_employees.rdd.mapPartitions(allocate_tasks).collect()\n",
    "\n",
    "# Debug: Print the results\n",
    "print(\"Task Assignments:\", results)\n",
    "\n",
    "# Format the results into a calendar\n",
    "calendar = pd.DataFrame(index=range(num_employees), columns=range(1, 32))\n",
    "calendar.index.name = 'Employee_ID'\n",
    "\n",
    "# Populate the calendar\n",
    "for assignment in results:\n",
    "    emp_id, task_id, start_date, duration = assignment\n",
    "    for day in range(start_date, start_date + duration):\n",
    "        if day in calendar.columns:\n",
    "            calendar.at[emp_id, day] = f'Task{task_id}'\n",
    "\n",
    "# Fill NaN with empty string\n",
    "calendar = calendar.fillna('')\n",
    "\n",
    "# Display the calendar\n",
    "import IPython.display as display\n",
    "display.display(calendar)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79eca33-bc83-485e-91ee-e30cd1cb657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 466, in __getnewargs__\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:466\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    467\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTEXT_ONLY_VALID_ON_DRIVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    468\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    469\u001b[0m     )\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m assignments\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Parallel processing using partition IDs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m all_assignments \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_partition\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Format the results into a calendar\u001b[39;00m\n\u001b[1;32m    142\u001b[0m calendar \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(num_employees), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5268\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5267\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5277\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5251\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5249\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5251\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5254\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from ortools.linear_solver import pywraplp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmployeeTaskAllocation\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate synthetic data for employees and tasks\n",
    "num_employees = 30\n",
    "num_tasks = 100\n",
    "locations = [\"Location1\", \"Location2\", \"Location3\"]\n",
    "skills = [\"Skill1\", \"Skill2\", \"Skill3\"]\n",
    "\n",
    "# Create synthetic employees data\n",
    "employees = []\n",
    "for i in range(num_employees):\n",
    "    employees.append({\n",
    "        'id': i,\n",
    "        'locations': random.sample(locations, random.randint(1, len(locations))),  # Random locations\n",
    "        'skills': random.sample(skills, random.randint(1, len(skills))),  # Random skills\n",
    "        'holidays': random.sample(range(1, 31), random.randint(1, 5))  # Random holidays in a month\n",
    "    })\n",
    "\n",
    "# Create synthetic tasks data\n",
    "tasks = []\n",
    "for i in range(num_tasks):\n",
    "    tasks.append({\n",
    "        'id': i,\n",
    "        'location': random.choice(locations),  # Single location for each task\n",
    "        'skills_required': random.sample(skills, random.randint(1, len(skills))),  # Random required skills\n",
    "        'start_date': random.randint(1, 15),  # Random start date in the first half of the month\n",
    "        'duration': random.randint(1, 5),  # Random duration between 1 and 5 days\n",
    "        'working_hours_per_day': random.randint(1, 8)  # Random working hours per day between 1 and 8\n",
    "    })\n",
    "\n",
    "# Convert the synthetic data to Spark DataFrames\n",
    "df_employees = spark.createDataFrame(employees)\n",
    "df_tasks = spark.createDataFrame(tasks)\n",
    "\n",
    "# Save tasks to Delta Lake to track assignment status\n",
    "df_tasks.withColumn(\"status\", col(\"id\") % 10) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/delta/tasks\")\n",
    "\n",
    "# Function to solve the task allocation problem for a partition of tasks\n",
    "def allocate_tasks(partition, employee_list):\n",
    "    num_employees = len(employee_list)\n",
    "    task_list = list(partition)\n",
    "    num_tasks = len(task_list)\n",
    "    \n",
    "    solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "    assignment_vars = {}\n",
    "\n",
    "    for emp_id in range(num_employees):\n",
    "        for task_id in range(num_tasks):\n",
    "            assignment_vars[emp_id, task_id] = solver.BoolVar(f'x[{emp_id},{task_id}]')\n",
    "\n",
    "    for task_idx, task in enumerate(task_list):\n",
    "        task_id = task['id']\n",
    "        solver.Add(solver.Sum(\n",
    "            assignment_vars[emp_id, task_idx] for emp_id in range(num_employees)\n",
    "            if task['location'] in employee_list[emp_id]['locations'] and\n",
    "            all(skill in employee_list[emp_id]['skills'] for skill in task['skills_required']) and\n",
    "            task['start_date'] not in employee_list[emp_id]['holidays']\n",
    "        ) == 1)\n",
    "\n",
    "    for emp_id in range(num_employees):\n",
    "        for day in range(1, 32):\n",
    "            overlapping_tasks = [\n",
    "                task_idx for task_idx, task in enumerate(task_list)\n",
    "                if task['start_date'] <= day < task['start_date'] + task['duration']\n",
    "            ]\n",
    "            solver.Add(solver.Sum(\n",
    "                assignment_vars[emp_id, task_idx] * task['working_hours_per_day']\n",
    "                for task_idx in overlapping_tasks\n",
    "            ) <= 8)\n",
    "\n",
    "    for emp_id in range(num_employees):\n",
    "        for week in range(5):\n",
    "            weekly_tasks = [\n",
    "                task_idx for task_idx, task in enumerate(task_list)\n",
    "                if task['start_date'] // 7 == week or\n",
    "                (task['start_date'] + task['duration']) // 7 == week\n",
    "            ]\n",
    "            solver.Add(solver.Sum(\n",
    "                assignment_vars[emp_id, task_idx] * task['working_hours_per_day']\n",
    "                for task_idx in weekly_tasks\n",
    "            ) <= 40)\n",
    "\n",
    "    solver.Maximize(solver.Sum(\n",
    "        assignment_vars[emp_id, task_idx] for emp_id in range(num_employees) for task_idx in range(num_tasks)\n",
    "    ))\n",
    "\n",
    "    status = solver.Solve()\n",
    "\n",
    "    assignments = []\n",
    "    if status == pywraplp.Solver.OPTIMAL:\n",
    "        for emp_id in range(num_employees):\n",
    "            for task_idx, task in enumerate(task_list):\n",
    "                if assignment_vars[emp_id, task_idx].solution_value() > 0:\n",
    "                    assignments.append((employee_list[emp_id]['id'], task['id'], task['start_date'], task['duration']))\n",
    "    return assignments\n",
    "\n",
    "# Broadcast employee data to all worker nodes\n",
    "employee_list = df_employees.collect()\n",
    "employee_broadcast = spark.sparkContext.broadcast(employee_list)\n",
    "\n",
    "# Read tasks from Delta Lake and distribute the processing using PySpark\n",
    "delta_tasks = DeltaTable.forPath(spark, \"/tmp/delta/tasks\")\n",
    "\n",
    "def process_partition(partition_id):\n",
    "    # Fetch the partition of tasks for this partition ID\n",
    "    task_subset = delta_tasks.toDF().filter(col(\"status\") == partition_id).collect()\n",
    "    if not task_subset:\n",
    "        return []\n",
    "    \n",
    "    # Solve the allocation problem for this partition\n",
    "    assignments = allocate_tasks(task_subset, employee_broadcast.value)\n",
    "    \n",
    "    # Update Delta table to mark assigned tasks\n",
    "    assigned_task_ids = [task[1] for task in assignments]\n",
    "    delta_tasks.update(\n",
    "        condition=col(\"id\").isin(assigned_task_ids),\n",
    "        set={\"status\": \"-1\"}  # Mark as assigned\n",
    "    )\n",
    "    \n",
    "    return assignments\n",
    "\n",
    "# Parallel processing using partition IDs\n",
    "all_assignments = spark.sparkContext.parallelize(range(10), 10).flatMap(process_partition).collect()\n",
    "\n",
    "# Format the results into a calendar\n",
    "calendar = pd.DataFrame(index=range(num_employees), columns=range(1, 32))\n",
    "calendar.index.name = 'Employee_ID'\n",
    "\n",
    "# Populate the calendar\n",
    "for assignment in all_assignments:\n",
    "    emp_id, task_id, start_date, duration = assignment\n",
    "    for day in range(start_date, start_date + duration):\n",
    "        if day in calendar.columns:\n",
    "            calendar.at[emp_id, day] = f'Task{task_id}'\n",
    "\n",
    "# Fill NaN with empty string\n",
    "calendar = calendar.fillna('')\n",
    "\n",
    "# Display the calendar\n",
    "import IPython.display as display\n",
    "display.display(calendar)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239da39-a78d-4807-87fc-68faac88fb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
